file_links = [
    {
        "title": "Attention Is All You Need",
        "url": "https://arxiv.org/pdf/1706.03762"
    },
    {
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "url": "https://arxiv.org/pdf/1810.04805v2"
    },
    {
        "title": "LLaMA: Open and Efficient Foundation Language Models",
        "url": "https://arxiv.org/pdf/2302.13971v1"
    },
    {
        "title": "Improving Language Understanding by Generative Pre-Training",
        "url": "https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"
    },
    {
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "url": "https://arxiv.org/pdf/1907.11692v1"
    },
    {
        "title": "LoRA: Low-Rank Adaptation of Large Language Models",
        "url": "https://arxiv.org/pdf/2106.09685v2"
    },
    {
        "title": "Universal Language Model Fine-tuning for Text Classification",
        "url": "https://arxiv.org/pdf/1801.06146v5"
    },
    {
        "title": "Regularizing and Optimizing LSTM Language Models",
        "url": "https://arxiv.org/pdf/1708.02182v1"
    },
    {
        "title": "End-To-End Memory Networks",
        "url": "https://arxiv.org/pdf/1503.08895v5"
    },
    {
        "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
        "url": "https://arxiv.org/pdf/1910.01108v4"
    },
    {
        "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "url": "https://arxiv.org/pdf/2312.00752v2"
    },
    {
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
        "url": "https://arxiv.org/pdf/2201.11903v6"
    },
]